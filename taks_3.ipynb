{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a74667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“° News Columns: ['Unnamed: 0', 'headline', 'url', 'publisher', 'date', 'stock']\n",
      "ðŸ“ˆ Processing C:\\Users\\HP\\Downloads\\yfinance_data\\yfinance_data\\AAPL_historical_data.csv\n",
      "ðŸ“Š Stock Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "ðŸ“ˆ Processing C:\\Users\\HP\\Downloads\\yfinance_data\\yfinance_data\\AMZN_historical_data.csv\n",
      "ðŸ“Š Stock Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "ðŸ“ˆ Processing C:\\Users\\HP\\Downloads\\yfinance_data\\yfinance_data\\GOOG_historical_data.csv\n",
      "ðŸ“Š Stock Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "ðŸ“ˆ Processing C:\\Users\\HP\\Downloads\\yfinance_data\\yfinance_data\\META_historical_data.csv\n",
      "ðŸ“Š Stock Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "ðŸ“ˆ Processing C:\\Users\\HP\\Downloads\\yfinance_data\\yfinance_data\\MSFT_historical_data.csv\n",
      "ðŸ“Š Stock Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "ðŸ“ˆ Processing C:\\Users\\HP\\Downloads\\yfinance_data\\yfinance_data\\NVDA_historical_data.csv\n",
      "ðŸ“Š Stock Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "ðŸ“ˆ Processing C:\\Users\\HP\\Downloads\\yfinance_data\\yfinance_data\\TSLA_historical_data.csv\n",
      "ðŸ“Š Stock Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "\n",
      "ðŸ“Š Correlation between sentiment and stock returns:\n",
      "AAPL_historical_data: -0.002\n",
      "AMZN_historical_data: -0.019\n",
      "GOOG_historical_data: 0.014\n",
      "META_historical_data: -0.006\n",
      "MSFT_historical_data: -0.013\n",
      "NVDA_historical_data: 0.009\n",
      "TSLA_historical_data: 0.028\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "\n",
    "# ---------- Load and Clean News ----------\n",
    "def load_news(news_path):\n",
    "    df_news = pd.read_csv(news_path)\n",
    "    print(\"ðŸ“° News Columns:\", df_news.columns.tolist())\n",
    "\n",
    "    # Fix if 'Date' instead of 'date'\n",
    "    if 'Date' in df_news.columns:\n",
    "        df_news.rename(columns={'Date': 'date'}, inplace=True)\n",
    "\n",
    "    if 'date' not in df_news.columns or 'headline' not in df_news.columns:\n",
    "        raise ValueError(\"âŒ News file must contain 'date' and 'headline' columns.\")\n",
    "\n",
    "    df_news['date'] = pd.to_datetime(df_news['date'], errors='coerce')\n",
    "    df_news = df_news.dropna(subset=['date', 'headline'])\n",
    "\n",
    "    from textblob import TextBlob\n",
    "    df_news['sentiment'] = df_news['headline'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "    df_news['date'] = df_news['date'].dt.date\n",
    "\n",
    "    sentiment_daily = df_news.groupby('date')['sentiment'].mean().reset_index()\n",
    "    return sentiment_daily\n",
    "\n",
    "\n",
    "def process_stock(stock_path, sentiment_df):\n",
    "    stock = pd.read_csv(stock_path)\n",
    "    print(f\"ðŸ“ˆ Processing {stock_path}\")\n",
    "    print(\"ðŸ“Š Stock Columns:\", stock.columns.tolist())\n",
    "\n",
    "    # Fix common column name variants\n",
    "    if 'Date' in stock.columns:\n",
    "        stock.rename(columns={'Date': 'date'}, inplace=True)\n",
    "    if 'Close' in stock.columns:\n",
    "        stock.rename(columns={'Close': 'close'}, inplace=True)\n",
    "\n",
    "    if 'date' not in stock.columns or 'close' not in stock.columns:\n",
    "        raise ValueError(f\"âŒ {stock_path} must have 'date' and 'close' columns.\")\n",
    "\n",
    "    stock['date'] = pd.to_datetime(stock['date'], errors='coerce')\n",
    "    stock = stock.dropna(subset=['date', 'close'])\n",
    "    stock['date'] = stock['date'].dt.date\n",
    "    stock = stock.sort_values('date')\n",
    "    stock['daily_return'] = stock['close'].pct_change()\n",
    "\n",
    "    merged = pd.merge(stock, sentiment_df, on='date', how='inner')\n",
    "    correlation = merged[['daily_return', 'sentiment']].corr().iloc[0, 1]\n",
    "\n",
    "    company_name = os.path.basename(stock_path).split('.')[0]\n",
    "    return company_name, correlation\n",
    "\n",
    "\n",
    "# ---------- Main Runner ----------\n",
    "def analyze_multiple_stocks(news_path, stock_folder):\n",
    "    sentiment_df = load_news(news_path)\n",
    "    \n",
    "    results = []\n",
    "    for file in os.listdir(stock_folder):\n",
    "        if file.endswith('.csv'):\n",
    "            path = os.path.join(stock_folder, file)\n",
    "            company, corr = process_stock(path, sentiment_df)\n",
    "            results.append((company, corr))\n",
    "    \n",
    "    print(\"\\nðŸ“Š Correlation between sentiment and stock returns:\")\n",
    "    for company, corr in results:\n",
    "        print(f\"{company}: {corr:.3f}\")\n",
    "\n",
    "\n",
    "# ---------- Run ----------\n",
    "# Replace with your actual paths\n",
    "news_file = r\"C:\\Users\\HP\\Downloads\\raw_analyst_ratings.csv\\raw_analyst_ratings.csv\"\n",
    "stock_dir = r\"C:\\Users\\HP\\Downloads\\yfinance_data\\yfinance_data\"\n",
    "\n",
    "analyze_multiple_stocks(news_file, stock_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18252063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nltk>=3.9 (from textblob)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\miniconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\miniconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.5.1)\n",
      "Collecting regex>=2021.8.3 (from nltk>=3.9->textblob)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\miniconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\miniconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 524.3/624.3 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 624.3/624.3 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Installing collected packages: regex, nltk, textblob\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6 textblob-0.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9741564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0368a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
